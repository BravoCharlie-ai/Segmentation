{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "import cv2, os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/home/aakashjuseja/anaconda3/envs/first-project/lib/python3.6/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.5 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.6\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=256\n",
    "classes=5\n",
    "image_dir='train_images'\n",
    "mask_dir='train_masks'\n",
    "data_shape = size*size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "import numpy as np\n",
    "from keras.utils import Sequence\n",
    "import random\n",
    "percent=0.80\n",
    "val_image_dir='val_images'\n",
    "val_mask_dir='val_masks'\n",
    "data_shape=size*size\n",
    "import os\n",
    "\n",
    "\n",
    "\n",
    "def hot_encoder(label):\n",
    "    mask = np.zeros((size, size, classes))\n",
    "    for c in range(classes):\n",
    "        mask[:,:,c] = (label==c).astype(int)\n",
    "        \n",
    "    return mask\n",
    "\n",
    "def reshape_labels(y):\n",
    "    return np.reshape(y, (data_shape, classes))\n",
    "    \n",
    "\n",
    "def data_gen(img_folder, mask_folder, batch_size):\n",
    "  c = 0\n",
    "  n = os.listdir(img_folder) #List of training images\n",
    "  random.shuffle(n)\n",
    "  \n",
    "  while (True):\n",
    "    img = np.zeros((batch_size,size , size, 3)).astype('float')\n",
    "    mask = np.zeros((batch_size, size*size, 5)).astype('float')\n",
    "\n",
    "    for i in range(c, c+batch_size): #initially from 0 to 16, c = 0. \n",
    "\n",
    "      train_img = cv2.imread(img_folder+'/'+n[i])/255.0\n",
    "      #train_img =  cv2.resize(train_img, (size, size))# Read an image from folder and resize\n",
    "\n",
    "      img[i-c] = train_img #add to array - img[0], img[1], and so on.\n",
    "    \n",
    "      #train_mask = cv2.imread(mask_folder+'/'+n[i], 0)\n",
    "\n",
    "      train_mask = hot_encoder(cv2.resize(cv2.imread(mask_folder+'/'+n[i], 0),(size,size)))\n",
    "      #train_mask = train_mask.reshape(512, 512, 1) # Add extra dimension for parity with train_img size [512 * 512 * 3]\n",
    "      train_mask=np.reshape(train_mask, (size*size, classes))\n",
    "      mask[i-c] = train_mask\n",
    "\n",
    "    c+=batch_size\n",
    "    if(c+batch_size>=len(os.listdir(img_folder))):\n",
    "      c=0\n",
    "      random.shuffle(n)\n",
    "                  # print \"randomizing again\"\n",
    "    yield img, mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Activation, Reshape, Permute\n",
    "from keras.layers import Convolution2D, MaxPooling2D, UpSampling2D, Conv2D, Conv2DTranspose,concatenate,Dropout\n",
    "from keras.layers import BatchNormalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmentation_model( input , output ):\n",
    "\n",
    "    img_input = input\n",
    "    o = output\n",
    "\n",
    "    o_shape = Model(img_input , o ).output_shape\n",
    "    i_shape = Model(img_input , o ).input_shape\n",
    "\n",
    "    if IMAGE_ORDERING == 'channels_first':\n",
    "        output_height = o_shape[2]\n",
    "        output_width = o_shape[3]\n",
    "        input_height = i_shape[2]\n",
    "        input_width = i_shape[3]\n",
    "        n_classes = o_shape[1]\n",
    "        o = (Reshape((  -1  , output_height*output_width   )))(o)\n",
    "        o = (Permute((2, 1)))(o)\n",
    "    elif IMAGE_ORDERING == 'channels_last':\n",
    "        output_height = o_shape[1]\n",
    "        output_width = o_shape[2]\n",
    "        input_height = i_shape[1]\n",
    "        input_width = i_shape[2]\n",
    "        n_classes = o_shape[3]\n",
    "        o = (Reshape((output_height*output_width , -1 )))(o)\n",
    "\n",
    "    o = (Activation('softmax'))(o)\n",
    "    model = Model( img_input , o )\n",
    "    #model.output_width = output_width\n",
    "    #model.output_height = output_height\n",
    "    #model.n_classes = n_classes\n",
    "    #model.input_height = input_height\n",
    "    #model.input_width = input_width\n",
    "    #model.model_name = \"\"\n",
    "\n",
    "    #model.train = MethodType( train , model )\n",
    "    #model.predict_segmentation = MethodType( predict , model )\n",
    "    #model.predict_multiple = MethodType( predict_multiple , model )\n",
    "    #model.evaluate_segmentation = MethodType( evaluate , model )\n",
    "\n",
    "\n",
    "    return model \n",
    "\n",
    "def resize_image( inp ,  s , data_format ):\n",
    "\n",
    "    try:\n",
    "\n",
    "        return Lambda( lambda x: K.resize_images(x, \n",
    "            height_factor=s[0], \n",
    "            width_factor=s[1], \n",
    "            data_format=data_format , \n",
    "            interpolation='bilinear') )( inp )\n",
    "\n",
    "    except Exception as e:\n",
    "\n",
    "        # if keras is old , then rely on the tf function ... sorry theono/cntk users . \n",
    "        assert data_format == 'channels_last'\n",
    "        assert IMAGE_ORDERING == 'channels_last'\n",
    "\n",
    "        import tensorflow as tf\n",
    "\n",
    "        return Lambda( \n",
    "            lambda x: tf.image.resize_images(\n",
    "                x , ( K.int_shape(x)[1]*s[0] ,K.int_shape(x)[2]*s[1] ))  \n",
    "            )( inp )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "# This code is proveded by Vladkryvoruchko and small modifications done by me .\n",
    "\n",
    "\n",
    "from math import ceil\n",
    "from keras import layers\n",
    "from keras.layers import Conv2D, MaxPooling2D, AveragePooling2D\n",
    "from keras.layers import BatchNormalization, Activation, Input, Dropout, ZeroPadding2D, Lambda\n",
    "from keras.layers.merge import Concatenate, Add\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.backend import tf as ktf\n",
    "import tensorflow as tf\n",
    "\n",
    "#data_format ='channels_last'\n",
    "IMAGE_ORDERING = 'channels_last'\n",
    "#from .model_utils import get_segmentation_model , resize_image\n",
    "\n",
    "\n",
    "learning_rate = 1e-3  # Layer specific learning rate\n",
    "# Weight decay not implemented\n",
    "\n",
    "\n",
    "def BN(name=\"\"):\n",
    "    return BatchNormalization(momentum=0.95, name=name, epsilon=1e-5)\n",
    "\n",
    "\n",
    "class Interp(layers.Layer):\n",
    "\n",
    "    def __init__(self, new_size, **kwargs):\n",
    "        self.new_size = new_size\n",
    "        super(Interp, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Interp, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        new_height, new_width = self.new_size\n",
    "        resized = ktf.image.resize_images(inputs, [new_height, new_width],\n",
    "                                          align_corners=True)\n",
    "        return resized\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return tuple([None, self.new_size[0], self.new_size[1], input_shape[3]])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(Interp, self).get_config()\n",
    "        config['new_size'] = self.new_size\n",
    "        return config\n",
    "\n",
    "\n",
    "# def Interp(x, shape):\n",
    "#    new_height, new_width = shape\n",
    "#    resized = ktf.image.resize_images(x, [new_height, new_width],\n",
    "#                                      align_corners=True)\n",
    "#    return resized\n",
    "\n",
    "\n",
    "def residual_conv(prev, level, pad=1, lvl=1, sub_lvl=1, modify_stride=False):\n",
    "    lvl = str(lvl)\n",
    "    sub_lvl = str(sub_lvl)\n",
    "    names = [\"conv\" + lvl + \"_\" + sub_lvl + \"_1x1_reduce\",\n",
    "             \"conv\" + lvl + \"_\" + sub_lvl + \"_1x1_reduce_bn\",\n",
    "             \"conv\" + lvl + \"_\" + sub_lvl + \"_3x3\",\n",
    "             \"conv\" + lvl + \"_\" + sub_lvl + \"_3x3_bn\",\n",
    "             \"conv\" + lvl + \"_\" + sub_lvl + \"_1x1_increase\",\n",
    "             \"conv\" + lvl + \"_\" + sub_lvl + \"_1x1_increase_bn\"]\n",
    "    if modify_stride is False:\n",
    "        prev = Conv2D(64 * level, (1, 1), strides=(1, 1), name=names[0],\n",
    "                      use_bias=False)(prev)\n",
    "    elif modify_stride is True:\n",
    "        prev = Conv2D(64 * level, (1, 1), strides=(2, 2), name=names[0],\n",
    "                      use_bias=False)(prev)\n",
    "\n",
    "    prev = BN(name=names[1])(prev)\n",
    "    prev = Activation('relu')(prev)\n",
    "\n",
    "    prev = ZeroPadding2D(padding=(pad, pad))(prev)\n",
    "    prev = Conv2D(64 * level, (3, 3), strides=(1, 1), dilation_rate=pad,\n",
    "                  name=names[2], use_bias=False)(prev)\n",
    "\n",
    "    prev = BN(name=names[3])(prev)\n",
    "    prev = Activation('relu')(prev)\n",
    "    prev = Conv2D(256 * level, (1, 1), strides=(1, 1), name=names[4],\n",
    "                  use_bias=False)(prev)\n",
    "    prev = BN(name=names[5])(prev)\n",
    "    return prev\n",
    "\n",
    "\n",
    "def short_convolution_branch(prev, level, lvl=1, sub_lvl=1, modify_stride=False):\n",
    "    lvl = str(lvl)\n",
    "    sub_lvl = str(sub_lvl)\n",
    "    names = [\"conv\" + lvl + \"_\" + sub_lvl + \"_1x1_proj\",\n",
    "             \"conv\" + lvl + \"_\" + sub_lvl + \"_1x1_proj_bn\"]\n",
    "\n",
    "    if modify_stride is False:\n",
    "        prev = Conv2D(256 * level, (1, 1), strides=(1, 1), name=names[0],\n",
    "                      use_bias=False)(prev)\n",
    "    elif modify_stride is True:\n",
    "        prev = Conv2D(256 * level, (1, 1), strides=(2, 2), name=names[0],\n",
    "                      use_bias=False)(prev)\n",
    "\n",
    "    prev = BN(name=names[1])(prev)\n",
    "    return prev\n",
    "\n",
    "\n",
    "def empty_branch(prev):\n",
    "    return prev\n",
    "\n",
    "\n",
    "def residual_short(prev_layer, level, pad=1, lvl=1, sub_lvl=1, modify_stride=False):\n",
    "    prev_layer = Activation('relu')(prev_layer)\n",
    "    block_1 = residual_conv(prev_layer, level,\n",
    "                            pad=pad, lvl=lvl, sub_lvl=sub_lvl,\n",
    "                            modify_stride=modify_stride)\n",
    "\n",
    "    block_2 = short_convolution_branch(prev_layer, level,\n",
    "                                       lvl=lvl, sub_lvl=sub_lvl,\n",
    "                                       modify_stride=modify_stride)\n",
    "    added = Add()([block_1, block_2])\n",
    "    return added\n",
    "\n",
    "\n",
    "def residual_empty(prev_layer, level, pad=1, lvl=1, sub_lvl=1):\n",
    "    prev_layer = Activation('relu')(prev_layer)\n",
    "\n",
    "    block_1 = residual_conv(prev_layer, level, pad=pad,\n",
    "                            lvl=lvl, sub_lvl=sub_lvl)\n",
    "    block_2 = empty_branch(prev_layer)\n",
    "    added = Add()([block_1, block_2])\n",
    "    return added\n",
    "\n",
    "\n",
    "def ResNet(inp, layers):\n",
    "    # Names for the first couple layers of model\n",
    "    names = [\"conv1_1_3x3_s2\",\n",
    "             \"conv1_1_3x3_s2_bn\",\n",
    "             \"conv1_2_3x3\",\n",
    "             \"conv1_2_3x3_bn\",\n",
    "             \"conv1_3_3x3\",\n",
    "             \"conv1_3_3x3_bn\"]\n",
    "\n",
    "    # Short branch(only start of network)\n",
    "\n",
    "    cnv1 = Conv2D(64, (3, 3), strides=(2, 2), padding='same', name=names[0],\n",
    "                  use_bias=False)(inp)  # \"conv1_1_3x3_s2\"\n",
    "    bn1 = BN(name=names[1])(cnv1)  # \"conv1_1_3x3_s2/bn\"\n",
    "    relu1 = Activation('relu')(bn1)  # \"conv1_1_3x3_s2/relu\"\n",
    "\n",
    "    cnv1 = Conv2D(64, (3, 3), strides=(1, 1), padding='same', name=names[2],\n",
    "                  use_bias=False)(relu1)  # \"conv1_2_3x3\"\n",
    "    bn1 = BN(name=names[3])(cnv1)  # \"conv1_2_3x3/bn\"\n",
    "    relu1 = Activation('relu')(bn1)  # \"conv1_2_3x3/relu\"\n",
    "\n",
    "    cnv1 = Conv2D(128, (3, 3), strides=(1, 1), padding='same', name=names[4],\n",
    "                  use_bias=False)(relu1)  # \"conv1_3_3x3\"\n",
    "    bn1 = BN(name=names[5])(cnv1)  # \"conv1_3_3x3/bn\"\n",
    "    relu1 = Activation('relu')(bn1)  # \"conv1_3_3x3/relu\"\n",
    "\n",
    "    res = MaxPooling2D(pool_size=(3, 3), padding='same',\n",
    "                       strides=(2, 2))(relu1)  # \"pool1_3x3_s2\"\n",
    "\n",
    "    # ---Residual layers(body of network)\n",
    "\n",
    "    \"\"\"\n",
    "    Modify_stride --Used only once in first 3_1 convolutions block.\n",
    "    changes stride of first convolution from 1 -> 2\n",
    "    \"\"\"\n",
    "\n",
    "    # 2_1- 2_3\n",
    "    res = residual_short(res, 1, pad=1, lvl=2, sub_lvl=1)\n",
    "    for i in range(2):\n",
    "        res = residual_empty(res, 1, pad=1, lvl=2, sub_lvl=i + 2)\n",
    "\n",
    "    # 3_1 - 3_3\n",
    "    res = residual_short(res, 2, pad=1, lvl=3, sub_lvl=1, modify_stride=True)\n",
    "    for i in range(3):\n",
    "        res = residual_empty(res, 2, pad=1, lvl=3, sub_lvl=i + 2)\n",
    "    if layers is 50:\n",
    "        # 4_1 - 4_6\n",
    "        res = residual_short(res, 4, pad=2, lvl=4, sub_lvl=1)\n",
    "        for i in range(5):\n",
    "            res = residual_empty(res, 4, pad=2, lvl=4, sub_lvl=i + 2)\n",
    "    elif layers is 101:\n",
    "        # 4_1 - 4_23\n",
    "        res = residual_short(res, 4, pad=2, lvl=4, sub_lvl=1)\n",
    "        for i in range(22):\n",
    "            res = residual_empty(res, 4, pad=2, lvl=4, sub_lvl=i + 2)\n",
    "    else:\n",
    "        print(\"This ResNet is not implemented\")\n",
    "\n",
    "    # 5_1 - 5_3\n",
    "    res = residual_short(res, 8, pad=4, lvl=5, sub_lvl=1)\n",
    "    for i in range(2):\n",
    "        res = residual_empty(res, 8, pad=4, lvl=5, sub_lvl=i + 2)\n",
    "\n",
    "    res = Activation('relu')(res)\n",
    "    return res\n",
    "\n",
    "\n",
    "def interp_block(prev_layer, level, feature_map_shape, input_shape):\n",
    "    if input_shape == (473, 473):#473\n",
    "        kernel_strides_map = {1: 60,\n",
    "                              2: 30,\n",
    "                              3: 20,\n",
    "                              6: 10}\n",
    "    elif input_shape == (713, 713):\n",
    "        kernel_strides_map = {1: 90,\n",
    "                              2: 45,\n",
    "                              3: 30,\n",
    "                              6: 15}\n",
    "    else:\n",
    "        print(\"Pooling parameters for input shape \",\n",
    "              input_shape, \" are not defined.\")\n",
    "        exit(1)\n",
    "\n",
    "    names = [\n",
    "        \"conv5_3_pool\" + str(level) + \"_conv\",\n",
    "        \"conv5_3_pool\" + str(level) + \"_conv_bn\"\n",
    "    ]\n",
    "    kernel = (kernel_strides_map[level], kernel_strides_map[level])\n",
    "    strides = (kernel_strides_map[level], kernel_strides_map[level])\n",
    "    prev_layer = AveragePooling2D(kernel, strides=strides)(prev_layer)\n",
    "    prev_layer = Conv2D(512, (1, 1), strides=(1, 1), name=names[0],\n",
    "                        use_bias=False)(prev_layer)\n",
    "    prev_layer = BN(name=names[1])(prev_layer)\n",
    "    prev_layer = Activation('relu')(prev_layer)\n",
    "    # prev_layer = Lambda(Interp, arguments={\n",
    "    #                    'shape': feature_map_shape})(prev_layer)\n",
    "    prev_layer = Interp(feature_map_shape)(prev_layer)\n",
    "    return prev_layer\n",
    "\n",
    "\n",
    "def build_pyramid_pooling_module(res, input_shape):\n",
    "    \"\"\"Build the Pyramid Pooling Module.\"\"\"\n",
    "    # ---PSPNet concat layers with Interpolation\n",
    "    feature_map_size = tuple(int(ceil(input_dim / 8.0))\n",
    "                             for input_dim in input_shape)\n",
    "\n",
    "\n",
    "    interp_block1 = interp_block(res, 1, feature_map_size, input_shape)\n",
    "    interp_block2 = interp_block(res, 2, feature_map_size, input_shape)\n",
    "    interp_block3 = interp_block(res, 3, feature_map_size, input_shape)\n",
    "    interp_block6 = interp_block(res, 6, feature_map_size, input_shape)\n",
    "\n",
    "    # concat all these layers. resulted\n",
    "    # shape=(1,feature_map_size_x,feature_map_size_y,4096)\n",
    "    res = Concatenate()([res,\n",
    "                         interp_block6,\n",
    "                         interp_block3,\n",
    "                         interp_block2,\n",
    "                         interp_block1])\n",
    "    return res\n",
    "\n",
    "\n",
    "def _build_pspnet(nb_classes, resnet_layers, input_shape, activation='softmax' ):\n",
    "    \n",
    "    assert IMAGE_ORDERING == 'channels_last'\n",
    "\n",
    "    inp = Input((input_shape[0], input_shape[1], 3))\n",
    "    \n",
    "    \n",
    "    res = ResNet(inp, layers=resnet_layers)\n",
    "    \n",
    "    psp = build_pyramid_pooling_module(res, input_shape)\n",
    "\n",
    "    x = Conv2D(512, (3, 3), strides=(1, 1), padding=\"same\", name=\"conv5_4\",\n",
    "               use_bias=False)(psp)\n",
    "    x = BN(name=\"conv5_4_bn\")(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "\n",
    "    x = Conv2D(nb_classes, (1, 1), strides=(1, 1), name=\"conv6\")(x)\n",
    "    # x = Lambda(Interp, arguments={'shape': (\n",
    "    #    input_shape[0], input_shape[1])})(x)\n",
    "    x = Interp([input_shape[0], input_shape[1]])(x)\n",
    "    \n",
    "\n",
    "    model = get_segmentation_model( inp  , x )\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=_build_pspnet(5, resnet_layers=101, input_shape=(473,473), activation='softmax' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_24 (InputLayer)           (None, 473, 473, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1_3x3_s2 (Conv2D)         (None, 237, 237, 64) 1728        input_24[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv1_1_3x3_s2_bn (BatchNormali (None, 237, 237, 64) 256         conv1_1_3x3_s2[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "activation_1185 (Activation)    (None, 237, 237, 64) 0           conv1_1_3x3_s2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2_3x3 (Conv2D)            (None, 237, 237, 64) 36864       activation_1185[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_2_3x3_bn (BatchNormalizat (None, 237, 237, 64) 256         conv1_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1186 (Activation)    (None, 237, 237, 64) 0           conv1_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv1_3_3x3 (Conv2D)            (None, 237, 237, 128 73728       activation_1186[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_3_3x3_bn (BatchNormalizat (None, 237, 237, 128 512         conv1_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1187 (Activation)    (None, 237, 237, 128 0           conv1_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_24 (MaxPooling2D) (None, 119, 119, 128 0           activation_1187[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1188 (Activation)    (None, 119, 119, 128 0           max_pooling2d_24[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce (Conv2D)     (None, 119, 119, 64) 8192        activation_1188[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1189 (Activation)    (None, 119, 119, 64) 0           conv2_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_292 (ZeroPadding (None, 121, 121, 64) 0           activation_1189[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_292[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1190 (Activation)    (None, 119, 119, 64) 0           conv2_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_1190[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj (Conv2D)       (None, 119, 119, 256 32768       activation_1188[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv2_1_1x1_proj_bn (BatchNorma (None, 119, 119, 256 1024        conv2_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_360 (Add)                   (None, 119, 119, 256 0           conv2_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv2_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1191 (Activation)    (None, 119, 119, 256 0           add_360[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce (Conv2D)     (None, 119, 119, 64) 16384       activation_1191[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1192 (Activation)    (None, 119, 119, 64) 0           conv2_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_293 (ZeroPadding (None, 121, 121, 64) 0           activation_1192[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_293[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1193 (Activation)    (None, 119, 119, 64) 0           conv2_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_1193[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_2_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_361 (Add)                   (None, 119, 119, 256 0           conv2_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1191[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1194 (Activation)    (None, 119, 119, 256 0           add_361[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce (Conv2D)     (None, 119, 119, 64) 16384       activation_1194[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_reduce_bn (BatchNor (None, 119, 119, 64) 256         conv2_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1195 (Activation)    (None, 119, 119, 64) 0           conv2_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_294 (ZeroPadding (None, 121, 121, 64) 0           activation_1195[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3 (Conv2D)            (None, 119, 119, 64) 36864       zero_padding2d_294[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_3x3_bn (BatchNormalizat (None, 119, 119, 64) 256         conv2_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1196 (Activation)    (None, 119, 119, 64) 0           conv2_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase (Conv2D)   (None, 119, 119, 256 16384       activation_1196[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2_3_1x1_increase_bn (BatchN (None, 119, 119, 256 1024        conv2_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_362 (Add)                   (None, 119, 119, 256 0           conv2_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1194[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1197 (Activation)    (None, 119, 119, 256 0           add_362[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce (Conv2D)     (None, 60, 60, 128)  32768       activation_1197[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1198 (Activation)    (None, 60, 60, 128)  0           conv3_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_295 (ZeroPadding (None, 62, 62, 128)  0           activation_1198[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_295[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1199 (Activation)    (None, 60, 60, 128)  0           conv3_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_1199[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj (Conv2D)       (None, 60, 60, 512)  131072      activation_1197[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv3_1_1x1_proj_bn (BatchNorma (None, 60, 60, 512)  2048        conv3_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_363 (Add)                   (None, 60, 60, 512)  0           conv3_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv3_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1200 (Activation)    (None, 60, 60, 512)  0           add_363[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_1200[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1201 (Activation)    (None, 60, 60, 128)  0           conv3_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_296 (ZeroPadding (None, 62, 62, 128)  0           activation_1201[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_296[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1202 (Activation)    (None, 60, 60, 128)  0           conv3_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_1202[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_2_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_364 (Add)                   (None, 60, 60, 512)  0           conv3_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1200[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1203 (Activation)    (None, 60, 60, 512)  0           add_364[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_1203[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1204 (Activation)    (None, 60, 60, 128)  0           conv3_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_297 (ZeroPadding (None, 62, 62, 128)  0           activation_1204[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_297[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1205 (Activation)    (None, 60, 60, 128)  0           conv3_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_1205[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_3_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_365 (Add)                   (None, 60, 60, 512)  0           conv3_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1203[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1206 (Activation)    (None, 60, 60, 512)  0           add_365[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce (Conv2D)     (None, 60, 60, 128)  65536       activation_1206[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_reduce_bn (BatchNor (None, 60, 60, 128)  512         conv3_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1207 (Activation)    (None, 60, 60, 128)  0           conv3_4_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_298 (ZeroPadding (None, 62, 62, 128)  0           activation_1207[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3 (Conv2D)            (None, 60, 60, 128)  147456      zero_padding2d_298[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_3x3_bn (BatchNormalizat (None, 60, 60, 128)  512         conv3_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1208 (Activation)    (None, 60, 60, 128)  0           conv3_4_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase (Conv2D)   (None, 60, 60, 512)  65536       activation_1208[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv3_4_1x1_increase_bn (BatchN (None, 60, 60, 512)  2048        conv3_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_366 (Add)                   (None, 60, 60, 512)  0           conv3_4_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1206[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1209 (Activation)    (None, 60, 60, 512)  0           add_366[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce (Conv2D)     (None, 60, 60, 256)  131072      activation_1209[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1210 (Activation)    (None, 60, 60, 256)  0           conv4_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_299 (ZeroPadding (None, 64, 64, 256)  0           activation_1210[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_299[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1211 (Activation)    (None, 60, 60, 256)  0           conv4_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_1211[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj (Conv2D)       (None, 60, 60, 1024) 524288      activation_1209[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv4_1_1x1_proj_bn (BatchNorma (None, 60, 60, 1024) 4096        conv4_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_367 (Add)                   (None, 60, 60, 1024) 0           conv4_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv4_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1212 (Activation)    (None, 60, 60, 1024) 0           add_367[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_1212[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1213 (Activation)    (None, 60, 60, 256)  0           conv4_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_300 (ZeroPadding (None, 64, 64, 256)  0           activation_1213[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_300[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1214 (Activation)    (None, 60, 60, 256)  0           conv4_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_1214[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_2_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_368 (Add)                   (None, 60, 60, 1024) 0           conv4_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1212[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1215 (Activation)    (None, 60, 60, 1024) 0           add_368[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_1215[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1216 (Activation)    (None, 60, 60, 256)  0           conv4_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_301 (ZeroPadding (None, 64, 64, 256)  0           activation_1216[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_301[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1217 (Activation)    (None, 60, 60, 256)  0           conv4_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_1217[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_3_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_369 (Add)                   (None, 60, 60, 1024) 0           conv4_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1215[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1218 (Activation)    (None, 60, 60, 1024) 0           add_369[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_1218[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_4_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1219 (Activation)    (None, 60, 60, 256)  0           conv4_4_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_302 (ZeroPadding (None, 64, 64, 256)  0           activation_1219[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_302[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_4_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1220 (Activation)    (None, 60, 60, 256)  0           conv4_4_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_1220[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_4_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_4_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_370 (Add)                   (None, 60, 60, 1024) 0           conv4_4_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1218[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1221 (Activation)    (None, 60, 60, 1024) 0           add_370[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_1221[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_5_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1222 (Activation)    (None, 60, 60, 256)  0           conv4_5_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_303 (ZeroPadding (None, 64, 64, 256)  0           activation_1222[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_303[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_5_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1223 (Activation)    (None, 60, 60, 256)  0           conv4_5_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_1223[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_5_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_5_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_371 (Add)                   (None, 60, 60, 1024) 0           conv4_5_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1221[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1224 (Activation)    (None, 60, 60, 1024) 0           add_371[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce (Conv2D)     (None, 60, 60, 256)  262144      activation_1224[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_reduce_bn (BatchNor (None, 60, 60, 256)  1024        conv4_6_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1225 (Activation)    (None, 60, 60, 256)  0           conv4_6_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_304 (ZeroPadding (None, 64, 64, 256)  0           activation_1225[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3 (Conv2D)            (None, 60, 60, 256)  589824      zero_padding2d_304[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_3x3_bn (BatchNormalizat (None, 60, 60, 256)  1024        conv4_6_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1226 (Activation)    (None, 60, 60, 256)  0           conv4_6_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase (Conv2D)   (None, 60, 60, 1024) 262144      activation_1226[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv4_6_1x1_increase_bn (BatchN (None, 60, 60, 1024) 4096        conv4_6_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_372 (Add)                   (None, 60, 60, 1024) 0           conv4_6_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1224[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1227 (Activation)    (None, 60, 60, 1024) 0           add_372[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce (Conv2D)     (None, 60, 60, 512)  524288      activation_1227[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_1_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1228 (Activation)    (None, 60, 60, 512)  0           conv5_1_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_305 (ZeroPadding (None, 68, 68, 512)  0           activation_1228[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_305[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_1_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1229 (Activation)    (None, 60, 60, 512)  0           conv5_1_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_1229[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj (Conv2D)       (None, 60, 60, 2048) 2097152     activation_1227[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_1_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_1_1x1_proj_bn (BatchNorma (None, 60, 60, 2048) 8192        conv5_1_1x1_proj[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_373 (Add)                   (None, 60, 60, 2048) 0           conv5_1_1x1_increase_bn[0][0]    \n",
      "                                                                 conv5_1_1x1_proj_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "activation_1230 (Activation)    (None, 60, 60, 2048) 0           add_373[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce (Conv2D)     (None, 60, 60, 512)  1048576     activation_1230[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_2_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1231 (Activation)    (None, 60, 60, 512)  0           conv5_2_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_306 (ZeroPadding (None, 68, 68, 512)  0           activation_1231[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_306[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_2_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1232 (Activation)    (None, 60, 60, 512)  0           conv5_2_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_1232[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_2_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_2_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_374 (Add)                   (None, 60, 60, 2048) 0           conv5_2_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1230[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1233 (Activation)    (None, 60, 60, 2048) 0           add_374[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce (Conv2D)     (None, 60, 60, 512)  1048576     activation_1233[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_reduce_bn (BatchNor (None, 60, 60, 512)  2048        conv5_3_1x1_reduce[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1234 (Activation)    (None, 60, 60, 512)  0           conv5_3_1x1_reduce_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "zero_padding2d_307 (ZeroPadding (None, 68, 68, 512)  0           activation_1234[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3 (Conv2D)            (None, 60, 60, 512)  2359296     zero_padding2d_307[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_3x3_bn (BatchNormalizat (None, 60, 60, 512)  2048        conv5_3_3x3[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "activation_1235 (Activation)    (None, 60, 60, 512)  0           conv5_3_3x3_bn[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase (Conv2D)   (None, 60, 60, 2048) 1048576     activation_1235[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_1x1_increase_bn (BatchN (None, 60, 60, 2048) 8192        conv5_3_1x1_increase[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "add_375 (Add)                   (None, 60, 60, 2048) 0           conv5_3_1x1_increase_bn[0][0]    \n",
      "                                                                 activation_1233[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "activation_1236 (Activation)    (None, 60, 60, 2048) 0           add_375[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_24 (AveragePo (None, 6, 6, 2048)   0           activation_1236[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_23 (AveragePo (None, 3, 3, 2048)   0           activation_1236[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_22 (AveragePo (None, 2, 2, 2048)   0           activation_1236[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_21 (AveragePo (None, 1, 1, 2048)   0           activation_1236[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool6_conv (Conv2D)     (None, 6, 6, 512)    1048576     average_pooling2d_24[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool3_conv (Conv2D)     (None, 3, 3, 512)    1048576     average_pooling2d_23[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool2_conv (Conv2D)     (None, 2, 2, 512)    1048576     average_pooling2d_22[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool1_conv (Conv2D)     (None, 1, 1, 512)    1048576     average_pooling2d_21[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool6_conv_bn (BatchNor (None, 6, 6, 512)    2048        conv5_3_pool6_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool3_conv_bn (BatchNor (None, 3, 3, 512)    2048        conv5_3_pool3_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool2_conv_bn (BatchNor (None, 2, 2, 512)    2048        conv5_3_pool2_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv5_3_pool1_conv_bn (BatchNor (None, 1, 1, 512)    2048        conv5_3_pool1_conv[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_1240 (Activation)    (None, 6, 6, 512)    0           conv5_3_pool6_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1239 (Activation)    (None, 3, 3, 512)    0           conv5_3_pool3_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1238 (Activation)    (None, 2, 2, 512)    0           conv5_3_pool2_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "activation_1237 (Activation)    (None, 1, 1, 512)    0           conv5_3_pool1_conv_bn[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "interp_29 (Interp)              (None, 60, 60, 512)  0           activation_1240[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "interp_28 (Interp)              (None, 60, 60, 512)  0           activation_1239[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "interp_27 (Interp)              (None, 60, 60, 512)  0           activation_1238[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "interp_26 (Interp)              (None, 60, 60, 512)  0           activation_1237[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_6 (Concatenate)     (None, 60, 60, 4096) 0           activation_1236[0][0]            \n",
      "                                                                 interp_29[0][0]                  \n",
      "                                                                 interp_28[0][0]                  \n",
      "                                                                 interp_27[0][0]                  \n",
      "                                                                 interp_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv5_4 (Conv2D)                (None, 60, 60, 512)  18874368    concatenate_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv5_4_bn (BatchNormalization) (None, 60, 60, 512)  2048        conv5_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1241 (Activation)    (None, 60, 60, 512)  0           conv5_4_bn[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 60, 60, 512)  0           activation_1241[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv6 (Conv2D)                  (None, 60, 60, 5)    2565        dropout_6[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "interp_30 (Interp)              (None, 473, 473, 5)  0           conv6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "reshape_65 (Reshape)            (None, 223729, 5)    0           interp_30[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation_1242 (Activation)    (None, 223729, 5)    0           reshape_65[0][0]                 \n",
      "==================================================================================================\n",
      "Total params: 46,766,789\n",
      "Trainable params: 46,708,165\n",
      "Non-trainable params: 58,624\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size=12\n",
    "train_gen = data_gen(image_dir,mask_dir, batch_size = batch_size)\n",
    "val_gen = data_gen(val_image_dir,val_mask_dir, batch_size =batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model=[]\n",
    "#from keras.models import load_model\n",
    "#model=load_model('PSPnet_C_001-19-0.56.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "opt=optimizers.Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "num_training_samples=np.array(os.listdir(image_dir)).shape[0]\n",
    "num_epochs=200\n",
    "num_validation_samples=np.array(os.listdir(val_image_dir)).shape[0]\n",
    "\n",
    "\n",
    "    # load data\n",
    "#(num_training_samples // batch_size)\n",
    "filepath = \"PSPnet_001-{epoch:02d}-{val_loss:.2f}.h5\"\n",
    "tb_cb = keras.callbacks.ModelCheckpoint(filepath, monitor='val_loss',save_best_only=True)\n",
    "\n",
    "model.fit_generator(generator=train_gen,\n",
    "                                      steps_per_epoch=(num_training_samples // batch_size),\n",
    "                                      epochs=num_epochs,\n",
    "                                      verbose=1,\n",
    "                                        validation_data=val_gen,\n",
    "                                        validation_steps=(num_validation_samples // batch_size),\n",
    "                                      use_multiprocessing=True,\n",
    "                                      workers=1,\n",
    "                                      max_queue_size=1,\n",
    "                                       callbacks=[tb_cb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
